# Transformer Model Overview

The **Transformer** is the foundational algorithm behind large language models (LLMs), providing a more advanced alternative to LSTMs. While LSTMs process text one word at a time, transformers handle entire paragraphs simultaneously, making them more efficient and capable of capturing complex language patterns.

## Key Features of Transformers

- **Released in 2017**: A breakthrough model for NLP tasks.
- **Handles Sequential Data**: Like LSTMs, transformers can process sequential data but in a more advanced way.
- **Self-Attention (Multi-Head Attention)**: The core innovation, allowing the model to focus on different parts of a sentence simultaneously.
- **Supports Supervised and Unsupervised Learning**: However, unsupervised learning must be converted into supervised for training purposes.
- **Byte-Pair Encoding (BPE)**: The embedding method used to convert words into numerical representations.

## Transformer Workflow

### 1. Input Embeddings
Each word in a sentence is transformed into a vector of multiple dimensions (e.g., 512 dimensions per word). These embeddings capture various aspects of the word's meaning.

### 2. Positional Encoding
Since transformers process all words at once, the model adds **positional encoding** to maintain the order of words in a sentence. This encoding uses sine and cosine functions for odd and even positions to preserve the sequence structure.

### 3. Multi-Head Attention
A key innovation of the transformer is **Multi-Head Attention**, which performs three steps:
- **Query**: The question we are asking.
- **Key**: Potential answers or context.
- **Value**: The closest or most relevant answer.

The model uses **cosine similarity** to compare the query and key, measuring how similar they are (ranging from -1 to +1).

### 4. Linear Transformation
The linear layer reduces the dimensions of the embeddings (e.g., from 512 to 256). If an activation function like ReLU is used after, it’s called a **Feed-Forward Network**.

### 5. Self-Attention Mechanism
In transformers, the same input is used as the query, key, and value in the **self-attention mechanism**, allowing the model to focus on different parts of the input sentence.

### 6. Matrix Multiplication
To calculate attention, the transformer performs matrix multiplication between the query and the key (with transposed dimensions). This results in a matrix where each value represents the attention weight between words.

### 7. Scaling and Softmax
The resulting attention scores are scaled and passed through a **softmax** function, which normalizes them to percentages. For example, if the attention score for a word is 99%, it means the model is focusing primarily on that word.

### 8. Multi-Head vs Single-Head Attention
Multiple attention heads allow the model to focus on different parts of the input simultaneously. For example, one head might focus on a person in an image, another on clouds, and a third on mountains. After processing, the outputs of all heads are concatenated and passed to the next layer.

### 9. Residual Connections
Transformers use **Add & Norm** steps to retain the original values of the input after each layer. This ensures the model doesn’t lose important information as it processes deeper layers.

## Encoder-Decoder Structure

- **Encoder**: Processes the input embeddings and generates context.
- **Decoder**: Takes the encoder output and generates the final output, such as a translated sentence. The decoder has two inputs:
  1. **Output Embedding**: The predicted output tokens (e.g., in a translation task).
  2. **Encoder Output**: The context generated by the encoder.

## Masked Multi-Head Attention (For Output)
The decoder uses **masked multi-head attention** to prevent the model from seeing future words during training. This is critical for tasks like language generation. The mask ensures that only the current and previous words are visible.

## Why Masking?
Masking hides future tokens during training to maintain the causal structure of the language. It ensures that the model predicts the next word based only on the words it has already seen.

---

### Example Use Cases of Transformers:
- **Translation**: Convert sentences from one language to another.
- **Text Summarization**: Generate summaries for long documents.
- **Question Answering**: Answer questions based on a passage of text.

---

### References:
- **Original Paper**: [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762)

---

By understanding the workflow and the components like multi-head attention and positional encoding, we can appreciate how transformers have revolutionized NLP tasks.
